"""
Search API using the unified search pipeline.

Features:
- Single unified search endpoint replacing multiple complex endpoints
- Proper chunk retrieval based on relevance
- Better context filtering and ranking
- Comprehensive logging and metrics
- Vector similarity search with graph-based context expansion
"""

import asyncio
import logging
import os
from typing import Optional, List
from datetime import datetime

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from neo4j import GraphDatabase
from dotenv import load_dotenv

from unified_search import get_search_pipeline, SearchScope, SearchResult
from utils import run_cypher_query_async

load_dotenv()

# Configuration
DB_URL = os.getenv("DB_URL")
DB_USERNAME = os.getenv("DB_USERNAME")
DB_PASSWORD = os.getenv("DB_PASSWORD")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
MAX_CHUNKS_PER_ANSWER = int(os.getenv("MAX_CHUNKS_PER_ANSWER", "7"))
QUICK_SEARCH_MAX_CHUNKS = int(os.getenv("QUICK_SEARCH_MAX_CHUNKS", "5"))

# Setup logging
logging.basicConfig(level=LOG_LEVEL)
logger = logging.getLogger("SearchAPI")

# Initialize Neo4j driver
driver = GraphDatabase.driver(DB_URL, auth=(DB_USERNAME, DB_PASSWORD))

# Pydantic models
class SearchRequest(BaseModel):
    question: str = Field(..., description="The question to search for")
    conversation_history: Optional[str] = Field(None, description="Previous conversation context")
    doc_id: Optional[str] = Field(None, description="Specific document ID to search within")
    scope: Optional[str] = Field("hybrid", description="Search scope: global, local, or hybrid")
    max_chunks: Optional[int] = Field(MAX_CHUNKS_PER_ANSWER, description="Maximum number of chunks to use in answer")

class SearchResponse(BaseModel):
    answer: str
    confidence_score: float
    chunks_used: int
    entities_found: List[str]
    search_time: float
    metadata: dict

class QuickSearchRequest(BaseModel):
    question: str = Field(..., description="Quick question for simple search")
    doc_id: Optional[str] = Field(None, description="Optional document ID")

# API Router
router = APIRouter()


def _normalize_doc_id(raw_doc_id: Optional[str]) -> Optional[str]:
    """Clean up doc_id values that come from autogenerated examples."""

    if not raw_doc_id:
        return None

    cleaned = raw_doc_id.strip()
    if not cleaned or cleaned.lower() in {"string", "none", "null"}:
        return None

    return cleaned

@router.post("/search", response_model=SearchResponse)
async def unified_search(request: SearchRequest):
    """
    Main search endpoint using MCP-based question classification and intelligent routing.
    
    This endpoint provides intelligent, context-aware search results by:
    
    1. **Question Classification (via MCP)**: Uses MCP (Model Context Protocol) server to classify
       questions into three types:
       - **BROAD**: Questions requiring overview/understanding → Uses map-reduce with community summaries
       - **CHUNK**: Questions requiring specific details → Uses chunk-level vector similarity search
       - **OUT_OF_SCOPE**: Questions not answerable from knowledge base → Returns polite fallback
    
    2. **Intelligent Routing**: Based on classification, routes to appropriate search strategy:
       - Broad questions: Map-reduce processing across multiple community summaries
       - Specific questions: Vector similarity search with graph-based context expansion
    
    3. **Advanced Retrieval**:
       - Native Neo4j vector index search for fast similarity matching
       - Graph-based context expansion via entity relationships
       - Graph-aware reranking (entity overlap, community relevance, centrality)
       - MMR (Maximal Marginal Relevance) diversity reranking
    
    4. **Answer Generation**: Uses LLM with retrieved context to generate comprehensive answers
    
    **MCP Integration**: When `USE_MCP_CLASSIFIER=true`, classification is performed via MCP server
    for improved accuracy and consistency. Falls back to direct LLM/heuristic classification if MCP unavailable.
    
    Args:
        request: SearchRequest containing question, optional conversation history, doc_id filter, and scope
    
    Returns:
        SearchResponse with answer, confidence score, chunks used, entities found, and metadata
    
    Example:
        ```json
        {
          "question": "What are the main policies in the document?",
          "conversation_history": "Previous context...",
          "doc_id": "optional-document-id",
          "scope": "hybrid",
          "max_chunks": 7
        }
        ```
    """
    try:
        # Validate scope
        scope_mapping = {
            "global": SearchScope.GLOBAL,
            "local": SearchScope.LOCAL,
            "hybrid": SearchScope.HYBRID
        }
        
        scope = scope_mapping.get(request.scope.lower(), SearchScope.HYBRID)

        # Clean up placeholder doc_id values
        normalized_doc_id = _normalize_doc_id(request.doc_id)
        
        # Get search pipeline
        search_pipeline = get_search_pipeline(driver)
        
        # Perform search
        result: SearchResult = await search_pipeline.search(
            question=request.question,
            conversation_history=request.conversation_history,
            doc_id=normalized_doc_id,
            scope=scope,
            max_chunks=request.max_chunks
        )
        
        # Extract entity names for response
        entity_names = [entity.get("name", "") for entity in result.entities_found]
        
        # Log search for analytics
        await _log_search_analytics(request, result, normalized_doc_id)
        
        return SearchResponse(
            answer=result.answer,
            confidence_score=result.confidence_score,
            chunks_used=len(result.relevant_chunks),
            entities_found=entity_names,
            search_time=result.search_metadata.get("search_time", 0.0),
            metadata={
                "scope": scope.value,
                "doc_id": normalized_doc_id,
                "community_summaries_used": len(result.community_summaries),
                "chunks_retrieved": result.search_metadata.get("chunks_retrieved", 0),
                "processing_details": result.search_metadata
            }
        )
        
    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

@router.post("/quick_search")
async def quick_search(request: QuickSearchRequest):
    """
    Quick search endpoint for simple questions without conversation history.
    
    This is a simplified version of the main search endpoint optimized for:
    - Simple, standalone questions (no conversation context needed)
    - Faster response times (uses fewer chunks)
    - Direct question answering without complex routing
    
    **Note**: Still uses MCP classification (if enabled) for optimal routing, but with
    simplified processing for speed.
    
    Args:
        request: QuickSearchRequest with question and optional doc_id
    
    Returns:
        Simplified response with answer, confidence, and search time
    
    Example:
        ```json
        {
          "question": "What is the deadline?",
          "doc_id": "optional-document-id"
        }
        ```
    """
    try:
        search_pipeline = get_search_pipeline(driver)

        normalized_doc_id = _normalize_doc_id(request.doc_id)

        result: SearchResult = await search_pipeline.search(
            question=request.question,
            doc_id=normalized_doc_id,
            scope=SearchScope.HYBRID,
            max_chunks=QUICK_SEARCH_MAX_CHUNKS  # Fewer chunks for quick search
        )
        
        return {
            "answer": result.answer,
            "confidence": result.confidence_score,
            "search_time": result.search_metadata.get("search_time", 0.0)
        }
        
    except Exception as e:
        logger.error(f"Quick search error: {e}")
        raise HTTPException(status_code=500, detail=f"Quick search failed: {str(e)}")

@router.get("/search_suggestions")
async def get_search_suggestions(doc_id: Optional[str] = None):
    """
    Get search suggestions based on available content in the knowledge base.
    
    Generates sample questions based on entities found in the documents:
    - Extracts entity types (PERSON, ORGANIZATION, CONCEPT, etc.)
    - Generates contextual questions for each entity type
    - Filters by document if doc_id is provided
    
    Useful for helping users discover what questions they can ask.
    
    Args:
        doc_id: Optional document ID to filter suggestions to specific document
    
    Returns:
        List of suggested questions (max 10)
    
    Example:
        GET /api/search/search_suggestions?doc_id=abc-123
    """
    try:
        if doc_id:
            # Get entities and topics from specific document
            # Entities are now merged across documents, so filter by chunks from this doc_id
            query = """
            MATCH (e:Entity)-[:MENTIONED_IN {doc_id: $doc_id}]->(:Chunk {doc_id: $doc_id})
            WITH DISTINCT e.type as entity_type, collect(DISTINCT e.name)[..5] as sample_entities
            RETURN entity_type, sample_entities
            ORDER BY entity_type
            """
            params = {"doc_id": doc_id}
        else:
            # Get general entities and topics
            query = """
            MATCH (e:Entity)
            WITH e.type as entity_type, collect(e.name)[..5] as sample_entities
            RETURN entity_type, sample_entities
            ORDER BY entity_type
            LIMIT 10
            """
            params = {}
        
        results = await run_cypher_query_async(driver, query, params)
        
        suggestions = []
        for result in results:
            entity_type = result["entity_type"]
            entities = result["sample_entities"]
            
            # Generate sample questions for this entity type
            if entity_type == "PERSON":
                suggestions.extend([f"What did {entity} do?" for entity in entities[:2]])
            elif entity_type == "ORGANIZATION":
                suggestions.extend([f"Tell me about {entity}" for entity in entities[:2]])
            elif entity_type == "CONCEPT":
                suggestions.extend([f"Explain {entity}" for entity in entities[:2]])
        
        return {"suggestions": suggestions[:10]}  # Limit to 10 suggestions
        
    except Exception as e:
        logger.error(f"Error getting search suggestions: {e}")
        return {"suggestions": [
            "What are the main topics in this document?",
            "Summarize the key points",
            "What are the important entities mentioned?"
        ]}

@router.get("/search_analytics")
async def get_search_analytics():
    """
    Get analytics about search usage and knowledge base statistics.
    
    Returns:
        - Knowledge base statistics (total chunks, documents, entities, summaries)
        - Search capabilities and features
        - Performance metrics
    
    Useful for monitoring system health and understanding knowledge base size.
    """
    try:
        # Get basic stats from Neo4j
        stats_query = """
        MATCH (c:Chunk)
        WITH count(c) as total_chunks, count(DISTINCT c.doc_id) as total_docs
        MATCH (e:Entity)
        WITH total_chunks, total_docs, count(e) as total_entities, 
             count(DISTINCT e.type) as entity_types
        MATCH (cs:CommunitySummary)
        RETURN total_chunks, total_docs, total_entities, entity_types, count(cs) as total_summaries
        """
        
        result = await run_cypher_query_async(driver, stats_query)
        stats = result[0] if result else {}
        
        return {
            "knowledge_base_stats": stats,
            "search_capabilities": {
                "supports_conversation_history": True,
                "supports_document_filtering": True,
                "supports_entity_search": True,
                "supports_semantic_search": True,
                "average_response_time": "< 2 seconds"
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting search analytics: {e}")
        return {"error": "Analytics temporarily unavailable"}

@router.post("/explain_search")
async def explain_search(request: SearchRequest):
    """
    Explain how a search would be processed without actually executing it.
    
    This endpoint provides transparency into the search pipeline by explaining:
    - How the question would be classified (BROAD/CHUNK/OUT_OF_SCOPE)
    - What retrieval strategy would be used
    - How context would be built and ranked
    - What answer generation method would be applied
    
    Useful for debugging and understanding search behavior.
    
    Args:
        request: SearchRequest with question and parameters
    
    Returns:
        Detailed explanation of search processing steps
    """
    try:
        explanation = {
            "query_processing": {
                "original_question": request.question,
                "will_rewrite_with_history": bool(request.conversation_history),
                "scope": request.scope,
                "document_filter": request.doc_id
            },
            "retrieval_strategy": {
                "primary_method": "vector_similarity_search",
                "will_expand_via_graph": request.scope in ["global", "hybrid"],
                "max_chunks_to_retrieve": request.max_chunks * 2,
                "similarity_threshold": 0.4
            },
            "context_building": {
                "will_include_community_summaries": True,
                "will_rank_by_relevance": True,
                "max_final_chunks": request.max_chunks
            },
            "answer_generation": {
                "method": "llm_with_context",
                "will_cite_sources": True,
                "includes_confidence_score": True
            }
        }
        
        return {"explanation": explanation}
        
    except Exception as e:
        logger.error(f"Error explaining search: {e}")
        raise HTTPException(status_code=500, detail="Error generating explanation")

async def _log_search_analytics(request: SearchRequest, result: SearchResult, normalized_doc_id: Optional[str]):
    """
    Log search analytics for monitoring and improvement
    """
    try:
        analytics_data = {
            "timestamp": datetime.now().isoformat(),
            "question_length": len(request.question),
            "has_conversation_history": bool(request.conversation_history),
            "doc_id_specified": bool(normalized_doc_id),
            "scope": request.scope,
            "chunks_used": len(result.relevant_chunks),
            "entities_found": len(result.entities_found),
            "confidence_score": result.confidence_score,
            "search_time": result.search_metadata.get("search_time", 0.0),
            "answer_length": len(result.answer)
        }
        
        logger.info(f"Search analytics: {analytics_data}")
        
    except Exception as e:
        logger.warning(f"Failed to log search analytics: {e}")

@router.get("/health")
async def search_health_check():
    """
    Search service health check endpoint.
    
    This is a search-specific health check that verifies:
    - Database connectivity (Neo4j)
    - Search pipeline readiness and initialization
    - Search service availability
    
    **Note**: For general API health, use `/health` endpoint.
    This endpoint focuses specifically on search functionality.
    
    Returns:
        Health status with timestamp, database status, and search pipeline status
    """
    try:
        # Test database connection
        test_query = "RETURN 1 as test"
        await run_cypher_query_async(driver, test_query)
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "database": "connected",
            "search_pipeline": "ready"
        }
        
    except Exception as e:
        logger.error(f"Search health check failed: {e}")
        raise HTTPException(status_code=503, detail="Search service unhealthy")
