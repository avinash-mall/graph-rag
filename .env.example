# =============================================================================
# Graph RAG API - Environment Configuration Template
# =============================================================================
# This file serves as a template for environment variables.
# Copy this file to .env and fill in your actual values, especially API keys.
#
# IMPORTANT: Never commit .env to version control. This .env.example file
# contains placeholders and is safe to commit.
# =============================================================================

# =============================================================================
# Application Configuration
# =============================================================================
# Host and port where the FastAPI application will run
APP_HOST=0.0.0.0
APP_PORT=8000

# Application metadata displayed in API documentation
APP_TITLE="Graph RAG API"
APP_DESCRIPTION="Graph RAG API combining document and search services"
APP_VERSION=1.0.0

# =============================================================================
# Graph and Neo4j Configuration
# =============================================================================
# Name of the graph database instance
GRAPH_NAME=entityGraph

# Neo4j database connection settings
# Format: bolt://host:port
# Default: bolt://localhost:7687 (for local Neo4j)
#          bolt://neo4j:7687 (for Docker Compose)
DB_URL=bolt://localhost:7687

# Neo4j authentication credentials
# CHANGE THESE in production!
DB_USERNAME=neo4j
DB_PASSWORD=neo4j123

# =============================================================================
# Neo4j Docker Configuration
# =============================================================================
# Docker image to use for Neo4j
# Options: neo4j:latest, neo4j:5.x, neo4j:enterprise
NEO4J_IMAGE=neo4j:latest

# Container name for Neo4j service
NEO4J_CONTAINER_NAME=neo4j

# Ports exposed by Neo4j container
# HTTP port for Neo4j Browser (default: 7474)
NEO4J_HTTP_PORT=7474
# Bolt protocol port (default: 7687)
NEO4J_BOLT_PORT=7687

# Docker restart policy
# Options: no, always, on-failure, unless-stopped
NEO4J_RESTART_POLICY=always

# Local path where Neo4j data will be persisted
# Relative to project root or absolute path
NEO4J_DATA_PATH=./neo4j_data

# Neo4j plugins to enable (JSON array format)
# Required plugins: graph-data-science (GDS), apoc
# GDS provides graph algorithms, APOC provides utility procedures
NEO4J_PLUGINS='["graph-data-science", "apoc"]'

# Neo4j authentication in format: username/password
NEO4J_AUTH=neo4j/neo4j123

# Security settings for Neo4j procedures
# Allows GDS procedures to run without restrictions
NEO4J_DBMS_SECURITY_PROCEDURES_UNRESTRICTED=gds.*
NEO4J_DBMS_SECURITY_PROCEDURES_WHITELIST=gds.*

# =============================================================================
# Neo4j Memory Configuration
# =============================================================================
# IMPORTANT: Adjust these values based on your system's available RAM
# Total heap + pagecache should not exceed available system memory
#
# Initial heap size for JVM
# Options: 512M, 1G, 2G, 4G, etc.
# Recommended: 512M-2G for development, 4G+ for production
NEO4J_DBMS_MEMORY_HEAP_INITIAL_SIZE=512M

# Maximum heap size for JVM
# Options: 1G, 2G, 4G, 8G, etc.
# Recommended: 1G-2G for development, 4G-8G for production
NEO4J_DBMS_MEMORY_HEAP_MAX_SIZE=1G

# Page cache size (used for caching frequently accessed data)
# Options: 512M, 1G, 2G, 4G, etc.
# Recommended: 512M-1G for development, 2G-4G for production
NEO4J_DBMS_MEMORY_PAGECACHE_SIZE=512M

# Maximum memory for transactions
# Options: 2G, 4G, 8G, etc.
# Recommended: 2G-4G for most use cases
NEO4J_DBMS_MEMORY_TRANSACTION_TOTAL_MAX=2G

# Docker container memory limits
# These set hard limits at the container level
# Options: 2g, 4g, 8g, etc.
# Should be at least: heap_max + pagecache + 1GB overhead
NEO4J_CONTAINER_MEM_LIMIT=2g
NEO4J_CONTAINER_MEMSWAP_LIMIT=2g

# =============================================================================
# LLM Provider Configuration (Main Language Model)
# =============================================================================
# LLM provider to use for answer generation and text processing
# Options:
#   - openai: OpenAI API or OpenAI-compatible APIs (Ollama, etc.)
#   - google: Google Gemini API
LLM_PROVIDER=google

# API key for the LLM provider
# For OpenAI: Get from https://platform.openai.com/api-keys
# For Google: Get from https://makersuite.google.com/app/apikey
# For Ollama: Leave empty (no key required for local instance)
OPENAI_API_KEY="YOUR_API_KEY_HERE"

# Model name to use
# OpenAI options: gpt-4, gpt-3.5-turbo, gpt-4-turbo-preview, etc.
# Google options: gemini-2.5-flash, gemini-pro, gemini-1.5-pro, etc.
# Ollama options: llama2, mistral, codellama, etc. (must be installed locally)
OPENAI_MODEL=gemini-2.5-flash

# Base URL for the LLM API
# OpenAI: https://api.openai.com/v1
# Google: https://generativelanguage.googleapis.com/v1beta
# Ollama (local): http://localhost:11434/v1
OPENAI_BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Temperature for text generation (0.0 = deterministic, 1.0 = creative)
# Range: 0.0 to 2.0
# Recommended: 0.0 for factual tasks, 0.7 for creative tasks
OPENAI_TEMPERATURE=0.0

# Stop sequences for text generation (JSON array format)
# Used to stop generation at specific tokens
# Ollama models:
OPENAI_STOP='["<|end_of_text|>", "<|end_header_id|>", "<|eot_id|>"]'
# Llamafile models (alternative):
# OPENAI_STOP='["<|end_of_text|>", "<|eot_id|>"]'

# =============================================================================
# Named Entity Recognition (NER) Configuration
# =============================================================================
# Model for entity extraction
# Options: Same as OPENAI_MODEL
# Recommended: Fast models like gemini-2.5-flash or gpt-3.5-turbo
NER_MODEL=gemini-2.5-flash

# Base URL for NER API (typically same as OPENAI_BASE_URL)
NER_BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# API key for NER service (typically same as OPENAI_API_KEY)
NER_API_KEY="YOUR_API_KEY_HERE"

# Temperature for NER extraction (lower = more consistent)
NER_TEMPERATURE=0.0

# Maximum number of retries for NER API calls
NER_MAX_RETRIES=3

# =============================================================================
# Coreference Resolution Configuration
# =============================================================================
# Model for resolving pronouns and references
# Options: Same as OPENAI_MODEL
COREF_MODEL=gemini-2.5-flash

# Base URL for coreference resolution API
COREF_BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# API key for coreference resolution
COREF_API_KEY="YOUR_API_KEY_HERE"

# Temperature for coreference resolution
COREF_TEMPERATURE=0.0

# Maximum word limit for coreference resolution (to avoid token limits)
COREF_WORD_LIMIT=8000

# =============================================================================
# Text Processing and Chunking Configuration
# =============================================================================
# Size of text chunks for graph processing (in tokens/characters)
# Recommended: 256, 512, 1024
# Smaller chunks = more granular search, larger chunks = more context
CHUNK_SIZE_GDS=512

# Chunk size for global search operations
GLOBAL_SEARCH_CHUNK_SIZE=512

# Number of top chunks to retrieve in global search
GLOBAL_SEARCH_TOP_N=30

# Batch size for processing chunks in global search
GLOBAL_SEARCH_BATCH_SIZE=10

# Number of documents to process in a single batch
# Lower values use less memory, higher values are faster
# Recommended: 10-50 depending on available RAM
DOCUMENT_PROCESSING_BATCH_SIZE=20

# =============================================================================
# Search and Retrieval Configuration
# =============================================================================
# Minimum relevance score threshold (0.0 to 1.0)
# Chunks below this threshold are filtered out
# Recommended: 0.3-0.5
RELEVANCE_THRESHOLD=0.40

# Maximum number of chunks to use when generating answers
CYPHER_QUERY_LIMIT=5

# Similarity threshold for fallback search methods
FALLBACK_SIMILARITY_THRESHOLD=0.0

# Jaro-Winkler similarity threshold for string matching fallback
# Range: 0.0 to 1.0, recommended: 0.8
FALLBACK_JARO_THRESHOLD=0.8

# Maximum entities to use in fallback queries
FALLBACK_ENTITY_LIMIT=3

# Maximum query results in fallback mode
FALLBACK_QUERY_LIMIT=3

# Relevance threshold for local (document-specific) search
LOCAL_SEARCH_RELEVANCE_THRESHOLD=0.3

# Number of top results for local search
LOCAL_SEARCH_TOP_K=3

# =============================================================================
# Embedding API Configuration
# =============================================================================
# IMPORTANT: URL format depends on provider
# - Gemini: Base URL without /embeddings (e.g., https://generativelanguage.googleapis.com/v1beta)
# - OpenAI/Ollama: Include /embeddings (e.g., http://localhost:11434/v1/embeddings)

# Embedding model name
# Google options: gemini-embedding-001, text-embedding-004
# OpenAI options: text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large
# Ollama options: mxbai-embed-large, nomic-embed-text, etc.
EMBEDDING_MODEL_NAME=gemini-embedding-001

# Base URL for embedding API
EMBEDDING_API_URL="https://generativelanguage.googleapis.com/v1beta"

# API key for embedding service
# For Gemini: Use same as OPENAI_API_KEY
# For OpenAI: Use same as OPENAI_API_KEY
# For Ollama: Leave empty
EMBEDDING_API_KEY="YOUR_API_KEY_HERE"

# Number of embeddings to generate in a single batch
# Higher values are faster but use more memory
# Recommended: 10-50
EMBEDDING_BATCH_SIZE=10

# =============================================================================
# API Timeout Configuration
# =============================================================================
# Timeout for API calls in seconds
# Recommended: 300-600 for long-running operations
API_TIMEOUT=600

# =============================================================================
# Relevance Scoring Configuration
# =============================================================================
# Maximum relevance score for reranking
RELEVANCE_SCORE_MAX=100

# Minimum relevance score for reranking
RELEVANCE_SCORE_MIN=1

# Epsilon value for cosine similarity calculations (prevents division by zero)
COSINE_EPSILON=1e-8

# Maximum characters to truncate in summaries
SUMMARY_TRUNCATE_CHARS=200

# =============================================================================
# Logging Configuration
# =============================================================================
# Directory for log files
LOG_DIR=logs

# Log file name
LOG_FILE=app.log

# Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Most verbose (development)
# INFO: Standard logging (production)
LOG_LEVEL=DEBUG

# =============================================================================
# Prompt Templates
# =============================================================================
# These prompts control how the LLM processes different tasks
# Modify these to customize behavior, but be careful with format requirements

# Prompt for extracting named entities
NER_EXTRACTION_PROMPT="Extract all named entities from the following text. For each entity, provide its name and type (e.g., cardinal value, date value, event name, building name, geo-political entity, language name, law name, money name, person name, organization name, location name, affiliation, ordinal value, percent value, product name, quantity value, time value, name of work of art). If uncertain, label it as 'UNKNOWN'. Return ONLY a valid Python list of dictionaries (without any additional text or markdown) in the exact format as example below: [{'name': 'Entity1', 'type': 'Type1'}, {'name': 'Entity2', 'type': 'Type3'}, ...].\n\n"

# System prompt for NER extraction
NER_SYSTEM_PROMPT="You are a professional NER extraction assistant who responds only in valid python list of dictionaries."

# Generic system message for general assistance
SYSTEM_MESSAGE_GENERIC="You are a professional assistant. Please provide a detailed answer."

# Prompt for reranking community summaries
RERANKING_PROMPT="You are an expert in evaluating textual information. Below are several community summaries. For the given user query, please rank these summaries in order of relevance from most to least relevant. For each summary, provide a JSON object containing:\n  - 'report_index': the index number (starting from 1) of the summary as listed below\n  - 'score': an integer between {RELEVANCE_SCORE_MIN} and {RELEVANCE_SCORE_MAX} indicating its relevance ({RELEVANCE_SCORE_MAX} being most relevant)\n Return the results as a JSON array sorted in descending order by score.\n User Query: {question}\n Community Summaries:\n"

# System prompt for reranking
RERANKING_SYSTEM_PROMPT="You are an expert at ranking textual information."

# System prompt for coreference resolution
COREF_SYSTEM_PROMPT="You are a professional text refiner skilled in coreference resolution."

# User prompt for coreference resolution
COREF_USER_PROMPT="Resolve all ambiguous pronouns and vague references in the following text by replacing them with their appropriate entities. Ensure no unresolved pronouns like 'he', 'she', 'himself', etc. remain. If the referenced entity is unclear, make an intelligent guess based on context.\n\n"

# System prompt for summarization
SUMMARY_SYSTEM_PROMPT="You are a professional summarization assistant. Do not use your prior knowledge and only use knowledge from the provided text."

# User prompt for summarization
SUMMARY_USER_PROMPT="Summarize the following text into a meaningful summary with key insights. If the text is too short or unclear, describe the entities and their possible connections. Avoid vague fillers.\n\n"

# System prompt for query rewriting
REWRITE_SYSTEM_PROMPT="You are a professional query rewriting assistant."

# User prompt for query rewriting
REWRITE_USER_PROMPT="Based on the previous conversation: '{conversation}', rewrite the query: '{question}' into a standalone query."

# System prompt for global search extraction
GSEARCH_EXTRACT_SYSTEM_PROMPT="You are a professional extraction assistant."

# User prompt for global search extraction
GSEARCH_EXTRACT_USER_PROMPT="You are an expert in extracting key points. For the following text chunks from a community summary, list the key points that are most relevant to answering the user query. For each key point, provide a brief description and assign a relevance rating between 1 and 100. Format each key point on a separate line in this format:\nKey point: <description> (Rating: <number>)"

# System prompt for global search reranking
GSEARCH_RERANK_SYSTEM_PROMPT="You are an expert at ranking key points."

# User prompt for global search reranking
GSEARCH_RERANK_USER_PROMPT="You are an expert at evaluating the relevance of key points extracted from community summaries. Below are key points extracted from the documents, each on a separate line. Please rerank them in descending order of relevance to the following user query, and output the reranked key points as a plain text list, each on a separate line.\nUser Query: {question}\nKey Points:\n{points}"

# System prompt for final answer generation in global search
GSEARCH_FINAL_SYSTEM_PROMPT="You are a professional assistant providing detailed answers."

# System prompt for local search
LSEARCH_SYSTEM_PROMPT="You are a professional assistant who answers strictly based on the provided context."

# User prompt for local search
LSEARCH_USER_PROMPT="You are a professional assistant who answers queries strictly based on the provided context.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:"

# System prompt for drift primer (initial broad search)
DRIFT_PRIMER_SYSTEM_PROMPT="You are a professional assistant."

# User prompt for drift primer
DRIFT_PRIMER_USER_PROMPT="You are an expert in synthesizing information from diverse community reports. Based on the following global context derived from document similarity filtering:\n{context}\n{conversation}\nPlease provide a preliminary answer to the following query and list any follow-up questions that could help refine it.\nFormat your response as plain text with the following sections:\n\nIntermediate Answer:\n<your intermediate answer here>\n\nFollow-Up Questions:\n1. <first>\n2. <second>\n..."

# System prompt for drift local (refined local search)
DRIFT_LOCAL_SYSTEM_PROMPT="You are a professional assistant."

# User prompt for drift local
DRIFT_LOCAL_USER_PROMPT="You are a professional assistant who refines queries using local document context. Based on the following local context:\n{local_context}\n{conversation}\nPlease provide an answer to the following follow-up query and list any additional follow-up questions.\nFormat your response as plain text with these sections:\n\nAnswer:\n<your answer here>\n\nFollow-Up Questions:\n1. <first>\n2. <second>\n...\n\nFollow-Up Query: {query}"

# System prompt for drift final (synthesis)
DRIFT_FINAL_SYSTEM_PROMPT="You are a professional assistant."

# User prompt for drift final
DRIFT_FINAL_USER_PROMPT="You are a professional assistant tasked with synthesizing a final detailed answer.\nBelow is the hierarchical data gathered:\n\nQuery: {query}\n\nIntermediate Answer: {intermediate}\n\nFollow-Up Interactions:\n{follow_ups}\n\nBased solely on the above, provide a final, comprehensive answer in plain text to the original query:\n{original_query}"

# =============================================================================
# Question Classifier Configuration
# =============================================================================
# Enable heuristic-based classification (keyword matching, patterns)
# Options: true, false
CLASSIFIER_USE_HEURISTICS=true

# Enable LLM-based classification (more accurate but slower)
# Options: true, false
CLASSIFIER_USE_LLM=true

# Use MCP (Model Context Protocol) classifier server
# Options: true, false
# If true, routes classification requests to MCP server
USE_MCP_CLASSIFIER=true

# =============================================================================
# MCP Classifier Server Configuration
# =============================================================================
# URL for MCP classifier server
# If running main app on host: http://localhost:8001/mcp
# If running main app in Docker: http://mcp-classifier-server:8001/mcp
MCP_CLASSIFIER_URL=http://localhost:8001/mcp

# Port for MCP classifier server
MCP_CLASSIFIER_PORT=8001

# Timeout for MCP classifier requests (seconds)
MCP_CLASSIFIER_TIMEOUT=30

# =============================================================================
# Map-Reduce Configuration (for Broad Questions)
# =============================================================================
# Maximum number of communities to process in map-reduce
# Higher values = more comprehensive but slower
MAP_REDUCE_MAX_COMMUNITIES=50

# Number of communities to process in each batch
MAP_REDUCE_BATCH_SIZE=5

# Minimum relevance score to include a community
# Range: 0.0 to 1.0
MAP_REDUCE_MIN_RELEVANCE=0.3

# Maximum communities to use in broad search
BROAD_SEARCH_MAX_COMMUNITIES=20

