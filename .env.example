# =============================================================================
# Graph RAG API - Environment Configuration Template
# =============================================================================
# This file serves as a template for environment variables.
# Copy this file to .env and fill in your actual values, especially API keys.
#
# IMPORTANT: Never commit .env to version control. This .env.example file
# contains placeholders and is safe to commit.
# =============================================================================

# =============================================================================
# Application Configuration
# =============================================================================
# Host and port where the FastAPI application will run
APP_HOST=0.0.0.0
APP_PORT=8000

# Application metadata displayed in API documentation
APP_TITLE="Graph RAG API"
APP_DESCRIPTION="Graph RAG API combining document and search services"
APP_VERSION=1.0.0

# =============================================================================
# Graph and Neo4j Configuration
# =============================================================================
# Name of the graph database instance
GRAPH_NAME=entityGraph

# Neo4j database connection settings
# Format: bolt://host:port
# Default: bolt://localhost:7687 (for local Neo4j)
#          bolt://neo4j:7687 (for Docker Compose)
DB_URL=bolt://localhost:7687

# Neo4j authentication credentials
# CHANGE THESE in production!
DB_USERNAME=neo4j
DB_PASSWORD=neo4j123

# =============================================================================
# Neo4j Docker Configuration
# =============================================================================
# Docker image to use for Neo4j
# Options: neo4j:latest, neo4j:5.x, neo4j:enterprise
NEO4J_IMAGE=neo4j:latest

# Container name for Neo4j service
NEO4J_CONTAINER_NAME=neo4j

# Ports exposed by Neo4j container
# HTTP port for Neo4j Browser (default: 7474)
NEO4J_HTTP_PORT=7474
# Bolt protocol port (default: 7687)
NEO4J_BOLT_PORT=7687

# Docker restart policy
# Options: no, always, on-failure, unless-stopped
NEO4J_RESTART_POLICY=always

# Local path where Neo4j data will be persisted
# Relative to project root or absolute path
NEO4J_DATA_PATH=./neo4j_data

# Neo4j plugins to enable (JSON array format)
# Required plugins: graph-data-science (GDS), apoc
# GDS provides graph algorithms, APOC provides utility procedures
NEO4J_PLUGINS='["graph-data-science", "apoc"]'

# Neo4j authentication in format: username/password
NEO4J_AUTH=neo4j/neo4j123

# Security settings for Neo4j procedures
# Allows GDS procedures to run without restrictions
NEO4J_DBMS_SECURITY_PROCEDURES_UNRESTRICTED=gds.*
NEO4J_DBMS_SECURITY_PROCEDURES_WHITELIST=gds.*

# =============================================================================
# Neo4j Memory Configuration
# =============================================================================
# IMPORTANT: Adjust these values based on your system's available RAM
# Total heap + pagecache should not exceed available system memory
#
# Initial heap size for JVM
# Options: 512M, 1G, 2G, 4G, etc.
# Recommended: 512M-2G for development, 4G+ for production
NEO4J_DBMS_MEMORY_HEAP_INITIAL_SIZE=512M

# Maximum heap size for JVM
# Options: 1G, 2G, 4G, 8G, etc.
# Recommended: 1G-2G for development, 4G-8G for production
NEO4J_DBMS_MEMORY_HEAP_MAX_SIZE=1G

# Page cache size (used for caching frequently accessed data)
# Options: 512M, 1G, 2G, 4G, etc.
# Recommended: 512M-1G for development, 2G-4G for production
NEO4J_DBMS_MEMORY_PAGECACHE_SIZE=512M

# Maximum memory for transactions
# Options: 2G, 4G, 8G, etc.
# Recommended: 2G-4G for most use cases
NEO4J_DBMS_MEMORY_TRANSACTION_TOTAL_MAX=2G

# Docker container memory limits
# These set hard limits at the container level
# Options: 2g, 4g, 8g, etc.
# Should be at least: heap_max + pagecache + 1GB overhead
NEO4J_CONTAINER_MEM_LIMIT=2g
NEO4J_CONTAINER_MEMSWAP_LIMIT=2g

# =============================================================================
# LLM Provider Configuration (Main Language Model)
# =============================================================================
# LLM provider to use for answer generation and text processing
# Options:
#   - openai: OpenAI API or OpenAI-compatible APIs (Ollama, etc.)
#   - google: Google Gemini API
LLM_PROVIDER=google

# API key for the LLM provider
# For OpenAI: Get from https://platform.openai.com/api-keys
# For Google: Get from https://makersuite.google.com/app/apikey
# For Ollama: Leave empty (no key required for local instance)
OPENAI_API_KEY="YOUR_API_KEY_HERE"

# Model name to use
# OpenAI options: gpt-4, gpt-3.5-turbo, gpt-4-turbo-preview, etc.
# Google options: gemini-2.5-flash, gemini-pro, gemini-1.5-pro, etc.
# Ollama options: llama2, mistral, codellama, etc. (must be installed locally)
OPENAI_MODEL=gemini-2.5-flash

# Base URL for the LLM API
# OpenAI: https://api.openai.com/v1
# Google: https://generativelanguage.googleapis.com/v1beta
# Cerebras: https://api.cerebras.ai/v1
# Ollama (local): http://localhost:11434/v1
OPENAI_BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# Temperature for text generation (0.0 = deterministic, 1.0 = creative)
# Range: 0.0 to 2.0
# Recommended: 0.0 for factual tasks, 0.7 for creative tasks
OPENAI_TEMPERATURE=0.0

# Stop sequences for text generation (JSON array format)
# Used to stop generation at specific tokens
# Ollama models:
OPENAI_STOP='["<|end_of_text|>", "<|end_header_id|>", "<|eot_id|>"]'
# Llamafile models (alternative):
# OPENAI_STOP='["<|end_of_text|>", "<|eot_id|>"]'

# =============================================================================
# Named Entity Recognition (NER) Configuration
# =============================================================================
# Model for entity extraction
# Options: Same as OPENAI_MODEL
# Recommended: Fast models like gemini-2.5-flash or gpt-3.5-turbo
NER_MODEL=gemini-2.5-flash

# Base URL for NER API (typically same as OPENAI_BASE_URL)
NER_BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# API key for NER service (typically same as OPENAI_API_KEY)
NER_API_KEY="YOUR_API_KEY_HERE"

# Temperature for NER extraction (lower = more consistent)
NER_TEMPERATURE=0.0

# Maximum number of retries for NER API calls
NER_MAX_RETRIES=3

# =============================================================================
# Coreference Resolution Configuration
# =============================================================================
# Model for resolving pronouns and references
# Options: Same as OPENAI_MODEL
COREF_MODEL=gemini-2.5-flash

# Base URL for coreference resolution API
COREF_BASE_URL="https://generativelanguage.googleapis.com/v1beta"

# API key for coreference resolution
COREF_API_KEY="YOUR_API_KEY_HERE"

# Temperature for coreference resolution
COREF_TEMPERATURE=0.0

# Maximum word limit for coreference resolution (to avoid token limits)
COREF_WORD_LIMIT=8000

# =============================================================================
# Text Processing and Chunking Configuration
# =============================================================================
# Size of text chunks for graph processing (in tokens/characters)
# Recommended: 256, 512, 1024
# Smaller chunks = more granular search, larger chunks = more context
CHUNK_SIZE_GDS=512

# Overlap between consecutive chunks (in characters)
# Helps maintain context across chunk boundaries
# Recommended: 50-100 (10-20% of chunk size)
CHUNK_OVERLAP=50

# Chunk size for global search operations
GLOBAL_SEARCH_CHUNK_SIZE=512

# Number of top chunks to retrieve in global search
GLOBAL_SEARCH_TOP_N=30

# Batch size for processing chunks in global search
GLOBAL_SEARCH_BATCH_SIZE=10

# Number of documents to process in a single batch
# Lower values use less memory, higher values are faster
# Recommended: 10-50 depending on available RAM
DOCUMENT_PROCESSING_BATCH_SIZE=20

# =============================================================================
# Search and Retrieval Configuration
# =============================================================================
# Minimum relevance score threshold (0.0 to 1.0)
# Chunks below this threshold are filtered out
# Recommended: 0.3-0.5
RELEVANCE_THRESHOLD=0.40

# Maximum number of chunks to use when generating answers
# Recommended: 5-10 for balanced performance
MAX_CHUNKS_PER_ANSWER=7

# Maximum number of chunks for quick search (fewer for faster responses)
# Recommended: 3-5
QUICK_SEARCH_MAX_CHUNKS=5

# Maximum number of community summaries to include in search context
# Recommended: 2-5
MAX_COMMUNITY_SUMMARIES=3

# Similarity threshold for chunk retrieval (0.0 to 1.0)
# Chunks with similarity below this are filtered out
# Recommended: 0.3-0.5
SIMILARITY_THRESHOLD_CHUNKS=0.4

# Similarity threshold for entity retrieval (0.0 to 1.0)
# Entities with similarity below this are filtered out
# Recommended: 0.5-0.7
SIMILARITY_THRESHOLD_ENTITIES=0.6

# Maximum number of results to return from Cypher queries
# Used to limit query result sets for performance
# Recommended: 5-20
CYPHER_QUERY_LIMIT=5

# Similarity threshold for fallback search methods
FALLBACK_SIMILARITY_THRESHOLD=0.0

# Jaro-Winkler similarity threshold for string matching fallback
# Range: 0.0 to 1.0, recommended: 0.8
FALLBACK_JARO_THRESHOLD=0.8

# Maximum entities to use in fallback queries
FALLBACK_ENTITY_LIMIT=3

# Maximum query results in fallback mode
FALLBACK_QUERY_LIMIT=3

# Relevance threshold for local (document-specific) search
LOCAL_SEARCH_RELEVANCE_THRESHOLD=0.3

# Number of top results for local search
LOCAL_SEARCH_TOP_K=3

# =============================================================================
# Embedding API Configuration
# =============================================================================
# IMPORTANT: URL format depends on provider
# - Gemini: Base URL without /embeddings (e.g., https://generativelanguage.googleapis.com/v1beta)
# - OpenAI/Ollama: Include /embeddings (e.g., http://localhost:11434/v1/embeddings, http://localhost:12434/engines/llama.cpp/v1/embeddings)

# Embedding provider type
# Options:
#   - "openai": OpenAI/Ollama compatible API (supports batch requests with array input)
#   - "google": Google Gemini embedding API
#   - "docker": Docker Model Runner (does NOT support batch - sends individual requests)
# Falls back to LLM_PROVIDER if not set
EMBEDDING_PROVIDER=openai

# Embedding model name
# Google options: gemini-embedding-001, text-embedding-004
# OpenAI options: text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large
# Ollama options: mxbai-embed-large, nomic-embed-text, ai/nomic-embed-text-v1.5 etc.
# Docker Model Runner: ai/nomic-embed-text-v1.5, etc.
EMBEDDING_MODEL_NAME=gemini-embedding-001

# Base URL for embedding API
# IMPORTANT: Docker networking considerations
# - If app runs in Docker and embedding service is on host: use host.docker.internal instead of localhost
#   Example: http://host.docker.internal:12434/engines/llama.cpp/v1/embeddings
# - If both run on host: use localhost
#   Example: http://localhost:11434/v1/embeddings
# - If both run in Docker: use Docker service name
#   Example: http://embedding-service:12434/engines/llama.cpp/v1/embeddings
EMBEDDING_API_URL="https://generativelanguage.googleapis.com/v1beta"

# API key for embedding service
# For Gemini: Use same as OPENAI_API_KEY
# For OpenAI: Use same as OPENAI_API_KEY
# For Ollama: Leave empty
EMBEDDING_API_KEY="YOUR_API_KEY_HERE"

# Embedding dimension (vector size)
# Common values:
#   - 768: text-embedding-004, gemini-embedding-001
#   - 1024: mxbai-embed-large
#   - 1536: text-embedding-3-small, text-embedding-ada-002 (OpenAI)
# This should match the dimension of your embedding model
EMBEDDING_DIMENSION=768

# Number of embeddings to generate in a single batch
# Higher values are faster but use more memory
# Recommended: 10-50
EMBEDDING_BATCH_SIZE=10

# =============================================================================
# API Timeout Configuration
# =============================================================================
# Timeout for API calls in seconds
# Recommended: 300-600 for long-running operations
API_TIMEOUT=600

# =============================================================================
# Relevance Scoring Configuration
# =============================================================================
# Maximum relevance score for reranking
RELEVANCE_SCORE_MAX=100

# Minimum relevance score for reranking
RELEVANCE_SCORE_MIN=1

# Epsilon value for cosine similarity calculations (prevents division by zero)
COSINE_EPSILON=1e-8

# Maximum characters to truncate in summaries
SUMMARY_TRUNCATE_CHARS=200

# =============================================================================
# Logging Configuration
# =============================================================================
# Directory for log files
LOG_DIR=logs

# Log file name
LOG_FILE=app.log

# Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Most verbose (development)
# INFO: Standard logging (production)
LOG_LEVEL=DEBUG

# =============================================================================
# Prompt Templates
# =============================================================================
# These prompts control how the LLM processes different tasks
# Modify these to customize behavior, but be careful with format requirements

# Prompt for extracting named entities
NER_EXTRACTION_PROMPT="Extract all named entities from the following text. For each entity, provide its name and type (e.g., cardinal value, date value, event name, building name, geo-political entity, language name, law name, money name, person name, organization name, location name, affiliation, ordinal value, percent value, product name, quantity value, time value, name of work of art). If uncertain, label it as 'UNKNOWN'. Return ONLY a valid Python list of dictionaries (without any additional text or markdown) in the exact format as example below: [{'name': 'Entity1', 'type': 'Type1'}, {'name': 'Entity2', 'type': 'Type3'}, ...].\n\n"

# System prompt for NER extraction
NER_SYSTEM_PROMPT="You are a professional NER extraction assistant who responds only in valid python list of dictionaries."

# Generic system message for general assistance
SYSTEM_MESSAGE_GENERIC="You are a professional assistant. Please provide a detailed answer."

# Prompt for reranking community summaries
RERANKING_PROMPT="You are an expert in evaluating textual information. Below are several community summaries. For the given user query, please rank these summaries in order of relevance from most to least relevant. For each summary, provide a JSON object containing:\n  - 'report_index': the index number (starting from 1) of the summary as listed below\n  - 'score': an integer between {RELEVANCE_SCORE_MIN} and {RELEVANCE_SCORE_MAX} indicating its relevance ({RELEVANCE_SCORE_MAX} being most relevant)\n Return the results as a JSON array sorted in descending order by score.\n User Query: {question}\n Community Summaries:\n"

# System prompt for reranking
RERANKING_SYSTEM_PROMPT="You are an expert at ranking textual information."

# System prompt for coreference resolution
COREF_SYSTEM_PROMPT="You are a professional text refiner skilled in coreference resolution."

# User prompt for coreference resolution
COREF_USER_PROMPT="Resolve all ambiguous pronouns and vague references in the following text by replacing them with their appropriate entities. Ensure no unresolved pronouns like 'he', 'she', 'himself', etc. remain. If the referenced entity is unclear, make an intelligent guess based on context.\n\n"

# System prompt for summarization
SUMMARY_SYSTEM_PROMPT="You are a professional summarization assistant. Do not use your prior knowledge and only use knowledge from the provided text."

# User prompt for summarization
SUMMARY_USER_PROMPT="Summarize the following text into a meaningful summary keeping maximum number of key insights. If the text is too short or unclear, describe the entities and their possible connections. Avoid vague fillers.\n\n"

# System prompt for query rewriting
REWRITE_SYSTEM_PROMPT="You are a professional query rewriting assistant."

# User prompt for query rewriting
REWRITE_USER_PROMPT="Based on the previous conversation: '{conversation}', rewrite the query: '{question}' into a standalone query."

# System prompt for global search extraction
GSEARCH_EXTRACT_SYSTEM_PROMPT="You are a professional extraction assistant."

# User prompt for global search extraction
GSEARCH_EXTRACT_USER_PROMPT="You are an expert in extracting key points. For the following text chunks from a community summary, list the key points that are most relevant to answering the user query. For each key point, provide a brief description and assign a relevance rating between 1 and 100. Format each key point on a separate line in this format:\nKey point: <description> (Rating: <number>)"

# System prompt for global search reranking
GSEARCH_RERANK_SYSTEM_PROMPT="You are an expert at ranking key points."

# User prompt for global search reranking
GSEARCH_RERANK_USER_PROMPT="You are an expert at evaluating the relevance of key points extracted from community summaries. Below are key points extracted from the documents, each on a separate line. Please rerank them in descending order of relevance to the following user query, and output the reranked key points as a plain text list, each on a separate line.\nUser Query: {question}\nKey Points:\n{points}"

# System prompt for final answer generation in global search
GSEARCH_FINAL_SYSTEM_PROMPT="You are a professional assistant providing detailed answers."

# System prompt for local search
LSEARCH_SYSTEM_PROMPT="You are a professional assistant who answers strictly based on the provided context."

# User prompt for local search
LSEARCH_USER_PROMPT="You are a professional assistant who answers queries strictly based on the provided context.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:"

# System prompt for drift primer (initial broad search)
DRIFT_PRIMER_SYSTEM_PROMPT="You are a professional assistant."

# User prompt for drift primer
DRIFT_PRIMER_USER_PROMPT="You are an expert in synthesizing information from diverse community reports. Based on the following global context derived from document similarity filtering:\n{context}\n{conversation}\nPlease provide a preliminary answer to the following query and list any follow-up questions that could help refine it.\nFormat your response as plain text with the following sections:\n\nIntermediate Answer:\n<your intermediate answer here>\n\nFollow-Up Questions:\n1. <first>\n2. <second>\n..."

# System prompt for drift local (refined local search)
DRIFT_LOCAL_SYSTEM_PROMPT="You are a professional assistant."

# User prompt for drift local
DRIFT_LOCAL_USER_PROMPT="You are a professional assistant who refines queries using local document context. Based on the following local context:\n{local_context}\n{conversation}\nPlease provide an answer to the following follow-up query and list any additional follow-up questions.\nFormat your response as plain text with these sections:\n\nAnswer:\n<your answer here>\n\nFollow-Up Questions:\n1. <first>\n2. <second>\n...\n\nFollow-Up Query: {query}"

# System prompt for drift final (synthesis)
DRIFT_FINAL_SYSTEM_PROMPT="You are a professional assistant."

# User prompt for drift final
DRIFT_FINAL_USER_PROMPT="You are a professional assistant tasked with synthesizing a final detailed answer.\nBelow is the hierarchical data gathered:\n\nQuery: {query}\n\nIntermediate Answer: {intermediate}\n\nFollow-Up Interactions:\n{follow_ups}\n\nBased solely on the above, provide a final, comprehensive answer in plain text to the original query:\n{original_query}"

# =============================================================================
# Question Classifier Configuration
# =============================================================================
# Enable heuristic-based classification (keyword matching, patterns)
# Options: true, false
CLASSIFIER_USE_HEURISTICS=true

# Enable LLM-based classification (more accurate but slower)
# Options: true, false
CLASSIFIER_USE_LLM=true

# Use MCP (Model Context Protocol) classifier server
# Options: true, false
# If true, routes classification requests to MCP server
USE_MCP_CLASSIFIER=true

# =============================================================================
# MCP Classifier Server Configuration
# =============================================================================
# URL for MCP classifier server
# If running main app on host: http://localhost:8001/mcp
# If running main app in Docker: http://mcp-classifier-server:8001/mcp
MCP_CLASSIFIER_URL=http://localhost:8001/mcp

# Port for MCP classifier server
MCP_CLASSIFIER_PORT=8001

# Timeout for MCP classifier requests (seconds)
MCP_CLASSIFIER_TIMEOUT=30

# =============================================================================
# MCP Neo4j Cypher Server Configuration
# =============================================================================
# Enable MCP Neo4j Cypher integration for graph/analytical queries
# This enables routing of GRAPH_ANALYTICAL questions to Neo4j Cypher queries
# Options: true, false
USE_MCP_NEO4J=true

# URL for MCP Neo4j Cypher server
# If running main app on host: http://localhost:8002/mcp
# If running main app in Docker: http://mcp-neo4j-server:8002/mcp
MCP_NEO4J_URL=http://localhost:8002/mcp

# Timeout for MCP Neo4j requests (seconds)
MCP_NEO4J_TIMEOUT=30

# Maximum number of refinement iterations for Cypher query generation
# The system will attempt to refine a Cypher query up to this many times
# if the initial query fails or returns no results
# Recommended: 3-5 iterations
MCP_NEO4J_MAX_REFINEMENT_ITERATIONS=3

# Entity Search Configuration
# Maximum number of search terms to process for entity matching
# Higher values process more terms but may be slower
# Recommended: 3-10
MCP_NEO4J_ENTITY_SEARCH_MAX_TERMS=5

# Default top_k for vector similarity search (number of results to retrieve)
# Higher values return more candidates but may include less relevant results
# Recommended: 5-20
MCP_NEO4J_ENTITY_VECTOR_SEARCH_TOP_K=10

# Multiplier for getting more candidates before filtering
# The system retrieves (top_k * multiplier) candidates, then filters by similarity threshold
# Higher values provide more candidates for filtering but use more resources
# Recommended: 2-3
MCP_NEO4J_ENTITY_VECTOR_SEARCH_CANDIDATE_MULTIPLIER=2

# Maximum results for text-based entity matching fallback
# Used when vector search fails or returns no results
# Recommended: 3-10
MCP_NEO4J_ENTITY_TEXT_MATCH_LIMIT=5

# Maximum results to include when formatting answers
# Limits the number of results sent to LLM for answer formatting
# Recommended: 10-20
MCP_NEO4J_ENTITY_RESULT_FORMAT_LIMIT=15

# Threshold for using simple formatting (number of results)
# If results count is below this threshold, uses simple formatting instead of LLM
# Recommended: 5-15
MCP_NEO4J_ENTITY_SIMPLE_FORMAT_THRESHOLD=10

# Maximum sample relationships to show in schema
# Number of relationship examples to include in schema description
# Recommended: 10-30
MCP_NEO4J_SCHEMA_SAMPLE_LIMIT=20

# Query Strategy Limits
# These control the maximum number of results returned by each query strategy
# Higher values return more results but may be slower

# Limit for fuzzy entity search strategy
# Recommended: 5-20
MCP_NEO4J_QUERY_FUZZY_SEARCH_LIMIT=10

# Limit for relationship exploration strategy
# Recommended: 10-30
MCP_NEO4J_QUERY_RELATIONSHIP_EXPLORATION_LIMIT=15

# Limit for path finding strategy
# Recommended: 5-20
MCP_NEO4J_QUERY_PATH_FINDING_LIMIT=10

# Limit for related entities strategy
# Recommended: 10-30
MCP_NEO4J_QUERY_RELATED_ENTITIES_LIMIT=20

# Limit for chunk context strategy
# Recommended: 3-10
MCP_NEO4J_QUERY_CHUNK_CONTEXT_LIMIT=5

# Maximum number of terms to use in query WHERE conditions
# Limits how many search terms are combined in a single query condition
# Recommended: 2-5
MCP_NEO4J_QUERY_MAX_TERMS_FOR_CONDITIONS=3

# =============================================================================
# Map-Reduce Configuration (for Broad Questions)
# =============================================================================
# Maximum number of communities to process in map-reduce
# Higher values = more comprehensive but slower
MAP_REDUCE_MAX_COMMUNITIES=50

# Number of communities to process in each batch
MAP_REDUCE_BATCH_SIZE=5

# Minimum relevance score to include a community
# Range: 0.0 to 1.0
MAP_REDUCE_MIN_RELEVANCE=0.3

# Maximum communities to use in broad search
BROAD_SEARCH_MAX_COMMUNITIES=20

# =============================================================================
# Explainability Configuration
# =============================================================================
# Enable inline citations and explainability block in LLM responses
# When enabled, the LLM will cite sources like [1], [2], and an
# "Explainability" section will be appended with references and graph trace
# Options: true, false
EXPLAIN_ENABLED=true

# Maximum number of sources to show in the references block
# Recommended: 5-10
EXPLAIN_MAX_SOURCES=8

# Maximum characters per source snippet in the references block
# Recommended: 200-400
EXPLAIN_SNIPPET_CHARS=320

# Only show sources that were actually cited by the LLM
# If false, shows top sources by relevance score instead
# Options: true, false
EXPLAIN_ONLY_CITED_SOURCES=true

# =============================================================================
# Resilience Configuration
# =============================================================================
# These settings control automatic retries and circuit breakers for external service calls
# All external API calls (LLM, embeddings, Neo4j, MCP) use these resilience patterns
# to handle transient failures gracefully without user-facing errors

# Maximum number of retry attempts for failed requests
# The system will retry failed requests up to this many times before giving up
# Recommended: 2-5 retries
RESILIENCE_MAX_RETRIES=3

# Exponential backoff multiplier for retries
# Delays between retries increase exponentially: initial_delay * (multiplier ^ retry_number)
# Example: With initial_delay=1.0 and multiplier=2.0, delays are: 1s, 2s, 4s, 8s
# Recommended: 1.5-3.0
RESILIENCE_BACKOFF_FACTOR=2.0

# Initial delay in seconds before first retry
# The first retry will wait this many seconds after the initial failure
# Recommended: 0.5-2.0 seconds
RESILIENCE_INITIAL_DELAY=1.0

# Circuit Breaker Configuration
# Circuit breakers prevent cascading failures by stopping requests to failing services

# Number of consecutive failures before circuit breaker opens
# When this threshold is reached, the circuit opens and requests are immediately rejected
# Recommended: 3-10 failures
RESILIENCE_CB_FAILURE_THRESHOLD=5

# Number of successful requests needed to close circuit breaker
# After the circuit opens, it needs this many successes to transition back to CLOSED
# Recommended: 1-3 successes
RESILIENCE_CB_SUCCESS_THRESHOLD=2

# Timeout in seconds before circuit breaker transitions from OPEN to HALF_OPEN
# After the circuit opens, it waits this long before testing if the service has recovered
# During HALF_OPEN state, a limited number of requests are allowed to test recovery
# Recommended: 30-120 seconds
RESILIENCE_CB_TIMEOUT=60.0

# Request timeout in seconds for individual requests
# All external service calls will timeout after this duration
# Prevents hanging operations and allows retries to kick in
# Recommended: 10-60 seconds (higher for long-running operations like document processing)
RESILIENCE_REQUEST_TIMEOUT=30.0

# =============================================================================
# Processing Configuration
# =============================================================================
# General batch size for processing operations
# Used for batch processing of various operations
# Recommended: 5-20 depending on available memory
BATCH_SIZE=10

# Maximum number of worker threads for parallel processing
# Controls parallelism for CPU-bound operations
# Recommended: 2-8 (typically number of CPU cores)
MAX_WORKERS=4

# Cache TTL in seconds for cached embeddings and results
# How long to cache embeddings and other computed results before refreshing
# Recommended: 3600 (1 hour) to 86400 (24 hours)
CACHE_TTL=3600

# =============================================================================
# Application Configuration
# =============================================================================
# Enable CORS (Cross-Origin Resource Sharing) for API endpoints
# Allows cross-origin requests from web browsers
# Set to false in production if API is only accessed from same origin
# Options: true, false
ENABLE_CORS=true

# Enable auto-reload for development
# Automatically restarts the server when code changes are detected
# Only use in development, disable in production
# Options: true, false
RELOAD=false

# =============================================================================
# Logging Configuration (Advanced)
# =============================================================================
# Enable JSON-formatted logs
# JSON logs are easier to parse and search in production environments
# Recommended: false for development, true for production
# Options: true, false
LOG_JSON=false

# Enable writing logs to file in addition to console
# When enabled, logs are written to both console and log file
# Log file location is controlled by LOG_DIR and LOG_FILE
# Options: true, false
LOG_TO_FILE=false
